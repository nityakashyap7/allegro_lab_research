# Relative Memorization Experiments â€“ Summer 2025

## ğŸ” Project Overview
This repository documents my contributions to a PhD thesis project on relative memorization at the Allegro Lab (PI: Robin Jia, PhD mentor: Johnny Wei). The lab has trained a suite of 8 large language models (LLMs) called the Hubble Models. My primary responsibilities include:
	â€¢	Inference & Validation: Running inference on the Hubble models to verify output correctness and analyze patterns of memorization.
	â€¢	Small Model Experiments: Training smaller-scale models on subsets of the Hubble dataset to determine whether they replicate the same relative memorization trends.

### Why This Matters
If relative memorization trends in smaller models mirror those in large-scale models, it suggests smaller models can act as cost-effective proxies for studying memorization. This would significantly lower the computational and financial barriers for AI companies to evaluate and mitigate memorization risks, which are critical for robustness, privacy, and legal compliance.


## ğŸ§  Research Objectives
- Quantify and visualize relative memorization patterns in Hubble Models by running inference
- Train a suite of smaller models on subsets of Hubble model training data
- Evaluate whether memorization patterns in smaller models align with those in the Hubble models
  

## ğŸ—ƒï¸ Repo Structure
.
â”œâ”€â”€ inference/                 Scripts for running inference on Hubble models
â”œâ”€â”€ training/                  Scripts for training smaller models
â”œâ”€â”€ preprocessing/             Tokenization, data cleaning, and metadata processing
â”œâ”€â”€ model_outputs/             CSVs and logs from inference runs
â”œâ”€â”€ data_analysis/             Notebooks and scripts for visualizing memorization trends
â”œâ”€â”€ notes/                     Experimental notes and observations
â””â”€â”€ README.md

## ğŸ“Š Key Results
T.B.A.


## ğŸ§° Tools & Skills Gained From This Project

### ğŸ”§ Core Tools & Libraries
  â€¢	HuggingFace Transformers â€“ Loading LLMs, running inference, tokenization
	â€¢	HuggingFace Datasets â€“ Loading .jsonl and remote datasets for NLP tasks
	â€¢	PyTorch â€“ Inference, tensor operations, batching
	â€¢	SLURM â€“ Job scheduling and management on an HPC cluster
	â€¢	Apptainer (Singularity) â€“ Running containerized jobs on a remote compute cluster

### ğŸ’» Environment & Workflow
  â€¢	vi/vim â€“ Editing scripts on remote machines
	â€¢	SSH config management â€“ Setting up named hosts for easy access to clusters
	â€¢	SCP â€“ Securely copying files to/from cluster environments
	â€¢	Remote Jupyter Notebooks â€“ Running and accessing .ipynb files over SSH tunnels

### ğŸ“‚ File & Code Management
  â€¢	GitHub version control â€“ Tracking progress, rolling back code, and syncing work
	â€¢	Organizing experiment outputs â€“ Using structured directories for inference logs, tokenized inputs, and CSV results

### ğŸ“Š Experimentation & Analysis
  â€¢	Relative memorization metrics â€“ Understanding and comparing memorization patterns across model scales
	â€¢	Per-example loss computation â€“ Custom loss evaluation for analysis
	â€¢	Metadata extraction from dataset â€“ Working with custom JSON fields like "meta" and "duplicates"

 ğŸ‘©â€ğŸ”¬ Mentorship
	â€¢	PhD Mentor: Johnny Wei
	â€¢	PI: Prof. Robin Jia
	â€¢	Lab: [AI, Language, Learning, Generalization, and Robustness (ALLeGRo) Lab](https://allegro-lab.github.io/)





